{
 "metadata": {
  "name": "",
  "signature": "sha256:2eaeb8d19c012bcef18d6b9809c714d011f0aee2da593186a96aceb89e137195"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\"\"\"\n",
      "Author: CHANDRAMOHAN T N\n",
      "File: PC-FTRL_W2V.py \n",
      "\"\"\"\n",
      "\n",
      "%matplotlib inline\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from sklearn.metrics import accuracy_score\n",
      "from sklearn.metrics import precision_recall_curve\n",
      "from sklearn.metrics import precision_recall_fscore_support\n",
      "from sklearn import metrics\n",
      "import sys\n",
      "\n",
      "def Get_data(i_f, W, word_idx_map, vocab):\n",
      "    f = open(i_f, 'r')\n",
      "    data = []\n",
      "    while 1:\n",
      "        line = f.readline()\n",
      "        line = line[0:-1]\n",
      "        if len(line) > 0:\n",
      "            items = map(int, line.split(','))\n",
      "            a = np.zeros((300, 1))\n",
      "            for i in range(len(items)):\n",
      "                w = vocab[i]\n",
      "                a = a + W[word_idx_map[w]]*word_idx_map[w]\n",
      "            data.append(a.tolist())\n",
      "        else:\n",
      "            f.close()\n",
      "            break\n",
      "    return data\n",
      "\n",
      "def Get_labels(i_f):\n",
      "    f = open(i_f, 'r')\n",
      "    labels = []\n",
      "    while 1:\n",
      "        line = f.readline()\n",
      "        line = line[0:-1]\n",
      "        if len(line) > 0:\n",
      "            labels.append(int(line))\n",
      "        else:\n",
      "            f.close\n",
      "            break\n",
      "    return labels\n",
      "\n",
      "def load_bin_vec(fname, vocab):\n",
      "    word_vecs = {}\n",
      "    with open(fname, \"rb\") as f:\n",
      "        header = f.readline()\n",
      "        vocab_size, layer1_size = map(int, header.split())\n",
      "        binary_len = np.dtype('float32').itemsize * layer1_size\n",
      "        for line in xrange(vocab_size):\n",
      "            word = []\n",
      "            while True:\n",
      "                ch = f.read(1)\n",
      "                if ch == ' ':\n",
      "                    word = ''.join(word)\n",
      "                    break\n",
      "                if ch != '\\n':\n",
      "                    word.append(ch)   \n",
      "            if word in vocab:\n",
      "               word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')  \n",
      "            else:\n",
      "                f.read(binary_len)\n",
      "    return word_vecs\n",
      "\n",
      "def add_unknown_words(word_vecs, vocab):\n",
      "    \"\"\"\n",
      "    For words that occur in at least min_df documents, create a separate word vector.    \n",
      "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
      "    \"\"\"\n",
      "    for word in vocab:\n",
      "        if word not in word_vecs:\n",
      "            word_vecs[word] = np.random.uniform(-0.25, 0.25, 300)\n",
      "    return word_vecs\n",
      "\n",
      "def get_W(word_vecs, k=300):\n",
      "    \"\"\"\n",
      "    Get word matrix. W[i] is the vector for word indexed by i\n",
      "    \"\"\"\n",
      "    vocab_size = len(word_vecs)\n",
      "    word_idx_map = dict()\n",
      "    W = np.zeros(shape=(vocab_size, k), dtype='float32')            \n",
      "    i = 0\n",
      "    for word in word_vecs:\n",
      "        W[i] = word_vecs[word]\n",
      "        word_idx_map[word] = i\n",
      "        i += 1\n",
      "    return W, word_idx_map\n",
      "\n",
      "def sigmoid(x):\n",
      "    return 1.0 / (1.0 + np.exp(-x))\n",
      "\n",
      "def log_likelihood(x, y, w):\n",
      "    p = sigmoid(np.dot(x, w))\n",
      "    loss = -y * np.log(p) - (1 - y) * np.log(1 - p)\n",
      "    return loss.mean()\n",
      "\n",
      "def gradient(x, y, w):\n",
      "    p = sigmoid(np.dot(x, w))\n",
      "    grad = np.dot(np.transpose(x), (p - y))\n",
      "    return grad.reshape(w.shape)\n",
      "\n",
      "def plot(a, b, c, x, l, n):\n",
      "    plt.plot(x, a, 'r', label='AUC Loss')\n",
      "    plt.plot(x, b, 'g', label='Log Loss')\n",
      "    plt.plot(x, c, 'b', label='Sq Error')\n",
      "    plt.title(l)\n",
      "    plt.legend(loc='upper right')\n",
      "    plt.ylabel('Loss')\n",
      "    if n == 1:\n",
      "        plt.xlabel('Iteration')\n",
      "        plt.savefig('PC-FTRL_W2V Test Loss.png')\n",
      "    else:\n",
      "        plt.xlabel('Lambda')\n",
      "        plt.savefig('PC-FTRL_W2V Performance.png')\n",
      "    plt.show()\n",
      "    plt.close()\n",
      "\n",
      "class PC_FTRL_W2V(object):\n",
      "\n",
      "    def __init__(self, x, y):\n",
      "        self.x = np.concatenate((np.ones((len(x), 1)), np.array(x)), axis=1)\n",
      "        self.y = np.array(y).reshape(len(y), 1)\n",
      "        self.w = np.zeros((self.x.shape[1], 1))\n",
      "        self.z = np.zeros((self.x.shape[1], 1))\n",
      "        self.gr = np.zeros((self.x.shape[1], 1))\n",
      "\n",
      "    def train_ogd(self, lmbda1, lmbda2, alpha, beta, idx):\n",
      "        x = self.x[idx * 1: (idx+1) * 1]\n",
      "        y = self.y[idx * 1: (idx+1) * 1]\n",
      "        g = gradient(x, y, self.w)\n",
      "        lr = (np.sqrt(self.gr + (g **2)) - np.sqrt(self.gr)) / alpha\n",
      "        self.z = self.z + g - lr * self.w\n",
      "        self.gr = self.gr + (g ** 2)\n",
      "        temp = np.zeros((self.w.shape[0], 1))\n",
      "        indx = ((np.absolute(self.z) > lmbda1).nonzero())[0]\n",
      "        for k in indx:\n",
      "            temp[k] = -(self.z[k] - np.sign(self.z[k]) * lmbda1) / ((beta + np.sqrt(self.gr[k])) / alpha + lmbda2)\n",
      "        self.w = temp\n",
      "        return self.w\n",
      "\n",
      "    def update_weights(self, w):\n",
      "        self.w = w\n",
      "\n",
      "    def save_model(self):\n",
      "        np.savez_compressed('Model_pc-ftrl_w2v', w=self.w, z=self.z)\n",
      "\n",
      "    def predict(self, x):\n",
      "        labels = [0 for i in xrange(len(x))]\n",
      "        p = sigmoid(np.dot(x, self.w))\n",
      "        for i in xrange(len(x)):\n",
      "            if p[i] > 0.5:\n",
      "                labels[i] = 1\n",
      "        return p, labels\n",
      "\n",
      "    def evaluate(self, y_pred, y_true, test_x):\n",
      "        fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred)\n",
      "        auc = metrics.auc(fpr, tpr)\n",
      "        auc_loss = 1 - auc\n",
      "        log_loss = log_likelihood(test_x, y_true, self.w)\n",
      "        sq_error = np.sum((y_pred - y_true)**2) / y_pred.shape[0]\n",
      "        return auc_loss, log_loss, sq_error\n",
      "\n",
      "    def perf_metrics(self, y_real, y_pred, y_prob):\n",
      "        precision, recall, thresholds = precision_recall_curve(y_real, y_prob)\n",
      "        plt.plot(precision, recall, 'r')\n",
      "        plt.ylabel('Precision')\n",
      "        plt.xlabel('Recall')\n",
      "        plt.savefig('PR_curve_pc-ftrl_w2v.png')\n",
      "        plt.show()\n",
      "        print('Accuracy:')\n",
      "        print(accuracy_score(y_real, y_pred))\n",
      "        print('Precison : Recall : F-score')\n",
      "        print(precision_recall_fscore_support(y_real, y_pred))\n",
      "\n",
      "def main():\n",
      "    v = np.load('../Data/books/Vocab_books.npz')\n",
      "    vocab = v['vocab']\n",
      "    w2v_file = '../Data/GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin'\n",
      "    w2v = load_bin_vec(w2v_file, vocab)\n",
      "    print('Word2Vec loaded ....')\n",
      "    print(\"num words already in word2vec: \" + str(len(w2v)))\n",
      "    w2v = add_unknown_words(w2v, vocab)\n",
      "    \n",
      "    W, word_idx_map = get_W(w2v)\n",
      "    i_f = '../Data/books/Train_x.csv'\n",
      "    train_x = Get_data(i_f, W, word_idx_map, vocab)\n",
      "    np.savez_compressed('Train_x', train_x='train_x')\n",
      "    i_f = '../Data/books/Test_x.csv'\n",
      "    test_x = Get_data(i_f, W, word_idx_map, vocab)\n",
      "    np.savez_compressed('Test_x', test_x='test_x')\n",
      "    a = np.load('Train_x.npz')\n",
      "    train_x = a['train_x']\n",
      "    print(train_x.shape)\n",
      "    a = np.load('Test_x.npz')\n",
      "    test_x = a['test_x']\n",
      "    print(test_x.shape)\n",
      "    test_x = np.concatenate((np.ones((len(test_x), 1)), np.array(test_x)), axis=1)\n",
      "    i_f = '../Data/books/Train_y.csv'\n",
      "    train_y = Get_labels(i_f)\n",
      "    i_f = '../Data/books/Test_y.csv'\n",
      "    test_y = Get_labels(i_f)\n",
      "    test_y = np.reshape(test_y, (test_x.shape[0], 1))\n",
      "\n",
      "    max_iter = 15000\n",
      "    alpha = 0.02\n",
      "    beta = 1.0\n",
      "    lmbda = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "    prev_auc_loss = sys.maxint\n",
      "    te_auc_loss = []\n",
      "    te_log_loss = []\n",
      "    te_sq_error = []\n",
      "    for l in lmbda:\n",
      "        print('Lambda2: ' + str(l))\n",
      "        pc_ftrl_reg = PC_FTRL_W2V(train_x, train_y)\n",
      "        tr_auc_loss = []\n",
      "        tr_log_loss = []\n",
      "        tr_sq_error = []\n",
      "        for i in range(max_iter):\n",
      "            idx = np.random.randint(len(train_x))\n",
      "            w = pc_ftrl_reg.train_ogd(1, l, alpha, beta, idx)\n",
      "            pc_ftrl_reg.update_weights(w)\n",
      "            if (i+1) % 500 == 0:\n",
      "                y_prob, y_pred = pc_ftrl_reg.predict(test_x)\n",
      "                a, b, c = pc_ftrl_reg.evaluate(y_prob, test_y, test_x)\n",
      "                tr_auc_loss.append(a)\n",
      "                tr_log_loss.append(b)\n",
      "                tr_sq_error.append(c)\n",
      "        y_prob, y_pred = pc_ftrl_reg.predict(test_x)\n",
      "        a, b, c = pc_ftrl_reg.evaluate(y_prob, test_y, test_x)\n",
      "        te_auc_loss.append(a)\n",
      "        te_log_loss.append(b)\n",
      "        te_sq_error.append(c)\n",
      "        if a < prev_auc_loss:\n",
      "            prev_auc_loss = a\n",
      "            best_l = l\n",
      "            pc_ftrl_reg.save_model()\n",
      "            r = [k for k in range(len(tr_auc_loss))]\n",
      "            plot(tr_auc_loss, tr_log_loss, tr_sq_error, r, 'PC-FTRL_W2V Test_loss, lambda2= ' + str(best_l), 1)\n",
      "    plot(te_auc_loss, te_log_loss, te_sq_error, lmbda, 'PC-FTRL_W2V Performance', 2)\n",
      "    a = np.load('Model_pc-ftrl_w2v.npz')\n",
      "    w = a['w']\n",
      "    pc_ftrl_reg.update_weights(w)\n",
      "    y_prob, y_pred = pc_ftrl_reg.predict(test_x)\n",
      "    pc_ftrl_reg.perf_metrics(test_y, y_pred, y_prob)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}